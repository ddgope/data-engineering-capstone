{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import boto3\n",
    "import configparser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('awskey.cfg'))\n",
    "CAPSTONE_AWS_ACCESS_KEY_ID = config.get('capstone', 'AWS_ACCESS_KEY_ID')\n",
    "CAPSTONE_AWS_SECRET_ACCESS_KEY = config.get('capstone', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_creds = {'region_name':\"us-west-2\",\n",
    "            'aws_access_key_id': CAPSTONE_AWS_ACCESS_KEY_ID,\n",
    "            'aws_secret_access_key': CAPSTONE_AWS_SECRET_ACCESS_KEY}\n",
    "                          \n",
    "client = boto3.client('s3', **s3_creds)\n",
    "resource = boto3.resource('s3', **s3_creds)\n",
    "bucket = resource.Bucket('us-immigration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The purpose of this project is to provide a deep dive into US immigration, primiarily focusing on the type of visas being issued and the profiles associated. The scope of this project is limited to the data sources listed below with data being aggregated across numerous features such as visatype, gender, port_of_entry, nationality and month.\n",
    "\n",
    "#### Data Description & Sources \n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office found [here](https://travel.trade.gov/research/reports/i94/historical/2016.html). Each report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries).\n",
    "- World Temperature Data: This dataset came from Kaggle found [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "- U.S. City Demographic Data: This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. Dataset comes from OpenSoft found [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "- Airport Code Table: This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia). It comes from [here](https://datahub.io/core/airport-codes#data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = resource.Bucket('us-immigration')\n",
    "#csv_files = [o.key for o in bucket.objects.filter(Prefix=\"Dimensions\") if '.csv' in o.key]\n",
    "csv_files = [o.key for o in bucket.objects.all() if '.csv' in o.key]\n",
    "#parquet_files = [o.key for o in bucket.objects.all() if '.parquet' in o.key]\n",
    "\n",
    "print(f\"{len(csv_files)} csv files\")\n",
    "#print(f\"{len(parquet_files)} parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for file in csv_files:\n",
    "    data[file.replace('.csv', '')] = pd.read_csv(client.get_object(Bucket='us-immigration', Key=file)['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in data.items():\n",
    "    k\n",
    "    v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['immigration_data_sample'].merge(data['visa_codes'], left_on='visatype', right_on='class_of_admission'\n",
    "                                     ).groupby(['gender','visatype'])[['cicid']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Data\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['airport_codes'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data['airport_codes']['coordinates'].head().str.split(',').tolist(), columns=['Latitude', 'Longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cities Data\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['us_cities_demographics'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['us_cities_demographics'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GlobalLandTemperaturesByCity'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GlobalLandTemperaturesByCity'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Data\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "!ls ../../data/18-83510-I94-Data-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/saurfang/spark-sas7bdat\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_apr16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_aug16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_dec16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_feb16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_jan16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_jul16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_jun16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_mar16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_may16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_nov16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_oct16/imm_data.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat\", forceLowercaseNames=True, inferLong=True)\n",
    "df_spark.write.parquet(\"i94_sep16/imm_data.parquet\",mode='overwrite',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,DoubleType,IntegerType,TimestampType,DateType\n",
    "from pyspark.sql.functions import udf, col\n",
    "import pyspark.sql.functions as funcToInt\n",
    "\n",
    "immigrationSchema = StructType([        \n",
    "        StructField('cicid', DoubleType()),\n",
    "        StructField('i94yr', DoubleType()),\n",
    "        StructField('i94mon', DoubleType()),\n",
    "        StructField('i94cit', DoubleType()),\n",
    "        StructField('i94res', DoubleType()),\n",
    "        StructField('i94port',StringType()),\n",
    "        StructField('arrdate',DoubleType()),\n",
    "        StructField('i94mode', DoubleType()),\n",
    "        StructField('i94addr', StringType()),        \n",
    "        StructField('depdate',DoubleType()),\n",
    "        StructField('i94bir', DoubleType()),\n",
    "        StructField('i94visa', DoubleType()),\n",
    "        StructField('count', DoubleType()),\n",
    "        StructField('dtadfile', StringType()),\n",
    "        StructField('visapost', StringType()), \n",
    "        StructField('occup', StringType()),\n",
    "        StructField('entdepa', StringType()), \n",
    "        StructField('entdepd', StringType()), \n",
    "        StructField('entdepu', StringType()), \n",
    "        StructField('matflag', StringType()), \n",
    "        StructField('biryear', DoubleType()),\n",
    "        StructField('dtaddto', StringType()), \n",
    "        StructField('gender', StringType()), \n",
    "        StructField('insnum', StringType()),   \n",
    "        StructField('airline', StringType()), \n",
    "        StructField('admnum', DoubleType()), \n",
    "        StructField('fltno', StringType()), \n",
    "        StructField('visatype',StringType())\n",
    "    ])\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_confg=sc._jsc.hadoopConfiguration()\n",
    "hadoop_confg.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_confg.set(\"fs.s3a.awsAccessKeyId\", CAPSTONE_AWS_ACCESS_KEY_ID)\n",
    "hadoop_confg.set(\"fs.s3a.awsSecretAccessKey\", CAPSTONE_AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_apr16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_apr16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_aug16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_aug16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_dec16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_dec16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_feb16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_feb16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_jan16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_jan16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_jul16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_jul16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_jun16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_jun16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_mar16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_mar16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_may16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_may16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_nov16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_nov16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_oct16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_oct16immigrationData.parquet\",mode='overwrite',compression='snappy')\n",
    "\n",
    "df_spark =spark.read.schema(immigrationSchema).parquet(\"i94_sep16/imm_data.parquet\")\n",
    "df_sparkFinal = df_spark.withColumn(\"cicid\", funcToInt.round(df_spark[\"cicid\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94yr\", funcToInt.round(df_spark[\"i94yr\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mon\", funcToInt.round(df_spark[\"i94mon\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94cit\", funcToInt.round(df_spark[\"i94cit\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94res\", funcToInt.round(df_spark[\"i94res\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"arrdate\", funcToInt.round(df_spark[\"arrdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94mode\", funcToInt.round(df_spark[\"i94mode\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"depdate\", funcToInt.round(df_spark[\"depdate\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94bir\", funcToInt.round(df_spark[\"i94bir\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"i94visa\", funcToInt.round(df_spark[\"i94visa\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"count\", funcToInt.round(df_spark[\"count\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"biryear\", funcToInt.round(df_spark[\"biryear\"], 1).cast('integer'))\\\n",
    "                        .withColumn(\"admnum\", funcToInt.round(df_spark[\"admnum\"], 1).cast('integer')) \n",
    "df_sparkFinal.write.parquet(\"s3a://us-immigration/i94_parquet_data/i94_sep16immigrationData.parquet\",mode='overwrite',compression='snappy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,DoubleType,IntegerType,TimestampType,DateType\n",
    "from pyspark.sql.functions import udf, col\n",
    "import pyspark.sql.functions as funcToInt\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_confg=sc._jsc.hadoopConfiguration()\n",
    "hadoop_confg.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_confg.set(\"fs.s3a.awsAccessKeyId\", CAPSTONE_AWS_ACCESS_KEY_ID)\n",
    "hadoop_confg.set(\"fs.s3a.awsSecretAccessKey\", CAPSTONE_AWS_SECRET_ACCESS_KEY)\n",
    "df = spark.read.parquet(\"s3a://us-immigration/i94_parquet_data/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('arrdate','depdate','dtadfile','dtaddto').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "s = \"01232017\"\n",
    "#s_datetime = datetime.datetime.strptime(s, '%Y%m%d')\n",
    "#s_datetime = datetime.datetime.strptime(s, '%m%d%Y')\n",
    "print(s_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i94yr: 4 digit year\n",
    "- i94mon: numeric month\n",
    "- i94cit & i94res: valid & invalid codes for processing\n",
    "- i94port: valid & invalid codes for processing\n",
    "- arrdate: arrival date in USA. SAS date numeric field.\n",
    "- i94mode: 1 = 'Air', 2 = 'Sea', 3 = 'Land', 9 = 'Not Reported'\n",
    "- i94addr: \n",
    "- i94bir: age of respondent in years\n",
    "- i94 visa: visa code, 1 = 'Business', 2 = 'Pleasure', 3 = 'Student'\n",
    "- count: summary statistics\n",
    "- dtadfile: character date field\n",
    "- visapost: department of state where visa was issued\n",
    "- occup: occupation performed in the U.S.\n",
    "- entdepa: arrival flag - admitted or paroled into the U.S.\n",
    "- entdepu: update flag - apprehended or overstayed, adjusted to perm residence\n",
    "- matflag: match flag - match of arrival and departure records\n",
    "- biryear: 4 digit year of birth\n",
    "- dtaddto: chracter date field - date to which admitted to U.S.\n",
    "- gender\n",
    "- insnum: INS number\n",
    "- airline: airline sued to arrive in U.S.\n",
    "- admnum - admission number\n",
    "- fltnoL flight number of airline used to arrive in U.S.\n",
    "- visatype: class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['gender','visatype']).agg({'count': 'count'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing Data\n",
    "Note: preprocessing was performed prior to storing CSV files in S3 buckets i.e. converting expanding columns, Capitalizing/Lowercasing test etc.\n",
    "#### Explore Data \n",
    "- Identify missing values\n",
    "- Identify duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in data.items():\n",
    "    null_count = v.isnull().sum()\n",
    "    if null_count.sum()>0:\n",
    "        ax = (null_count[null_count>0]/v.shape[0]).plot(kind='bar', title=f\"{k} Null %\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (data['airport_codes'][data['airport_codes'].iata_code.isnull()].type.value_counts()/\n",
    "      data['airport_codes'].type.value_counts()).plot(kind='bar', title=f\"Airport Type Null %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above provides further insight into the majority of null values in the airport data. Specifically, majority of null values associated with airport data stem from missing `iata_codes`. Upon further investigation it appears that the lesser important airports associated with immigration, namely, `balloon ports, closed, heliports, seaplanes and small airports` do not have corresponding `iata_codes` information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "- Either drop rows or fill missing data with median values where appropriate\n",
    "- Expand coordinates to Latitude & Longitude columns\n",
    "- Expand locations to City & State columns\n",
    "e.g. the data provided for `port_of_entry_codes` was originally `code` and `location`. These have subsequently been expanded out to `city` and `state_or_country` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['port_of_entry_codes'][['code', 'location']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data['port_of_entry_codes']['location'].head().str.split(',').tolist(), columns=['city', 'state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Model\n",
    "<img src=\"./images/schema.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Creating the data model involves various steps, which can be made significantly easier through the use of Airflow. The process of extracting files from S3 buckets, transforming the data and then writing CSV and PARQUET files to Redshift is accomplished through various tasks highlighted below in the ETL Dag graph. These steps include:\n",
    "- Extracting data from SAS Documents and writing as CSV files to S3 immigration bucket\n",
    "- Extracting remaining CSV and PARQUET files from S3 immigration bucket\n",
    "- Writing CSV and PARQUET files from S3 to Redshift\n",
    "- Performing data quality checks on the newly created tables\n",
    "<img src=\"./images/dag_graph.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data quality checks include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('awskey.cfg'))\n",
    "CAPSTONE_REDSHIFT_USER = config.get('redshift', 'CAPSTONE_REDSHIFT_USER')\n",
    "CAPSTONE_REDSHIFT_PASSWORD = config.get('redshift', 'CAPSTONE_REDSHIFT_PASSWORD')\n",
    "CAPSTONE_HOST = config.get('redshift', 'CAPSTONE_HOST')\n",
    "CAPSTONE_PORT = config.get('redshift', 'CAPSTONE_PORT')\n",
    "CAPSTONE_DB = config.get('redshift', 'CAPSTONE_DB')\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "eng = create_engine(f'postgres://{CAPSTONE_REDSHIFT_USER}:{CAPSTONE_REDSHIFT_PASSWORD}@{CAPSTONE_HOST}:{CAPSTONE_PORT}/{CAPSTONE_DB}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = eng.execute(\"\"\"\n",
    "SELECT TABLE_NAME\n",
    "FROM INFORMATION_SCHEMA.TABLES\n",
    "WHERE TABLE_TYPE = 'BASE TABLE' AND TABLE_CATALOG='dev'\n",
    "\"\"\")\n",
    "tables = pd.DataFrame([{**row} for row in qry]).tail(12)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qry = eng.execute(\"\"\"select * from i94mode limit 5;\"\"\")\n",
    "pd.DataFrame([{**row} for row in qry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['table_name'][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Distinct Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_from_table(table, col=0):\n",
    "    qry = eng.execute(f\"select * from {table} limit 1;\")\n",
    "    return pd.DataFrame([{**row} for row in qry]).columns[col]\n",
    "\n",
    "for table in tables['table_name'][1:]:\n",
    "    column = get_col_from_table(table)\n",
    "    sql = f\"SELECT COUNT(DISTINCT {column}) FROM {table};\"\n",
    "    c = eng.execute(sql).fetchone()[0]\n",
    "    print(f\"{table}:\\n\\t{c} distinct rows for column {column}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Study of Visa Counts by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = eng.execute(\"\"\"\n",
    "SELECT \n",
    "    im.gender, \n",
    "    im.visatype,\n",
    "    count(im.cicid) \n",
    "FROM immigration as im \n",
    "    JOIN visa_codes \n",
    "    ON im.visatype = visa_codes.class_of_admission \n",
    "GROUP BY im.gender, im.visatype\n",
    "\"\"\")\n",
    "\n",
    "qry_df = pd.DataFrame([{**row} for row in qry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qry_df.set_index(['visatype', 'gender']).unstack()[[('count', 'M'),('count', 'F')]]\n",
    "df.columns=['F', 'M']\n",
    "df.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data\n",
    "model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
